import os
import shutil
import openai
import requests
import json
import pypdf
from bs4 import BeautifulSoup
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_chroma import Chroma
from langchain.schema import Document
from langchain.document_loaders import PdyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains import RetrievalQA
from langchain.prompts import ChatPromptTemplate
from langchain.chains import LLMChain
from get_lunch_menu import lunch
os.environ["OPENAI_API_KEY"] = ""
client = openai.OpenAI()

class Chatbot:
    def __init__(self):
        """Chatbot í´ë˜ìŠ¤ ì´ˆê¸°í™”"""
        self.system_prompt = "ê¸°ë³¸ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ì…ë‹ˆë‹¤."
        self.llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0.2)
        self.vector_stores = {}

    def set_system_prompt(self, prompt):
        """ìš´ì˜ìê°€ System Prompt ì„¤ì •"""
        self.system_prompt = prompt

    def initialize_chroma(self, db_name):
        """ChromaDB ì´ˆê¸°í™” (ìš´ì˜ ë° êµìœ¡ DB ìƒì„±)"""
        chroma_db_path = f"./chroma_db_{db_name}"
        
        if os.path.exists(chroma_db_path):
            shutil.rmtree(chroma_db_path)
            print(f"ğŸ—‘ ê¸°ì¡´ ChromaDB ë°ì´í„° ì‚­ì œ ì™„ë£Œ! (DB: {db_name})")
        
        embeddings = OpenAIEmbeddings(model="text-embedding-3-large")
        self.vector_stores[db_name] = Chroma(embedding_function=embeddings, persist_directory=chroma_db_path)

    def is_pdf_encrypted(self, pdf_path):
        """PDF íŒŒì¼ì´ ì•”í˜¸í™”ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸"""
        try:
            with open(pdf_path, "rb") as file:
                reader = pypdf.PdfReader(file)
                return reader.is_encrypted
        except Exception as e:
            print(f":x: PDF ì•”í˜¸í™” í™•ì¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return False

    def process_pdf_dir_to_chroma(self, directory, db_name):
        """ë””ë ‰í† ë¦¬ ë‚´ ëª¨ë“  PDF íŒŒì¼ì„ ChromaDBì— ì €ì¥"""
        if db_name not in self.vector_stores:
            self.initialize_chroma(db_name)

        if not os.path.exists(directory):
            print(f":x: {directory} ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            return

        pdf_files = [f for f in os.listdir(directory) if f.endswith(".pdf")]

        if not pdf_files:
            print(f":warning: {directory}ì— PDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return

        for pdf_file in pdf_files:
            pdf_path = os.path.join(directory, pdf_file)
            print(f":page_facing_up: PDF ì²˜ë¦¬ ì¤‘: {pdf_file}")

            try:
                if self.is_pdf_encrypted(pdf_path):
                    print(f":x: {pdf_file}ì€(ëŠ”) ì•”í˜¸í™”ë˜ì–´ ìˆì–´ ì²˜ë¦¬í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    continue
                loader = PyPDFLoader(pdf_path)
                pages = loader.load()
            except Exception as e:
                print(f":x: PDF íŒŒì¼ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                continue

            corpus = Document(page_content="".join([page.page_content + "\n---\n" for page in pages]))
            text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
            chunks = text_splitter.split_documents([corpus])
            self.vector_stores[db_name].add_documents(chunks)
            print(f":white_check_mark: {pdf_file}ì—ì„œ {len(chunks)}ê°œ ë¬¸ì„œê°€ {db_name} ChromaDBì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")

    def scrape_and_store_skala(self, db_name):
        """SKALA ì›¹ì‚¬ì´íŠ¸ì—ì„œ ë°ì´í„° í¬ë¡¤ë§ í›„ ChromaDBì— ì €ì¥ (ìš´ì˜ ê´€ë ¨)"""
        if db_name not in self.vector_stores:
            self.initialize_chroma(db_name)

        url = "https://skala.co.kr/"
        headers = {"User-Agent": "Mozilla/5.0"}
        
        try:
            response = requests.get(url, headers=headers)
            response.raise_for_status()
        except requests.exceptions.RequestException as e:
            print(f":x: ì›¹ì‚¬ì´íŠ¸ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
            return

        soup = BeautifulSoup(response.text, "html.parser")

        sections = {
            "about": soup.find("section", {"class": "one_page_section _one_page_section one_page_section_1"}),
            "curriculum": soup.find("section", {"class": "one_page_section _one_page_section one_page_section_2"}),
            "faq": soup.find("section", {"class": "one_page_section _one_page_section one_page_section_3"}),
        }

        scraped_data = {
            key: section.get_text(separator="\n", strip=True) if section else "Not Found"
            for key, section in sections.items()
        }

        documents = [
            Document(page_content=value, metadata={"section": key})
            for key, value in scraped_data.items() if value != "Not Found"
        ]

        if documents:
            self.vector_stores[db_name].add_documents(documents)
            print(f":white_check_mark: í¬ë¡¤ë§ ë°ì´í„° {len(documents)}ê°œê°€ {db_name} ChromaDBì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")

    async def get_lunch_menu(self):
        return get_lunch_menu()

    def classify_query(self, user_prompt):
        """LLMì„ ì´ìš©í•´ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ìë™ìœ¼ë¡œ ë¶„ë¥˜"""
        classification_prompt = ChatPromptTemplate.from_template("""
        ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë¡œ ë¶„ë¥˜í•˜ì„¸ìš”: general, operation, education, lunch

        ì§ˆë¬¸: {question}
        
        ë‹µë³€ í˜•ì‹: [ë¶„ë¥˜]
        """)

        classification_chain = LLMChain(llm=self.llm, prompt=classification_prompt)
        result = classification_chain.run(question=user_prompt)
        
        category = result.strip().lower().replace("[", "").replace("]", "")
        print(f":pushpin: ì§ˆë¬¸ ë¶„ë¥˜ ê²°ê³¼: {category}")
        return category

    def query_rag(self, user_prompt, db_name):
        """ìš´ì˜ ë° êµìœ¡ ê´€ë ¨ ì§ˆë¬¸ì„ ChromaDB + LangChain RAGë¡œ ê²€ìƒ‰"""
        if db_name not in self.vector_stores:
            print(f":x: {db_name} ChromaDBê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë¨¼ì € ë°ì´í„° êµ¬ì¶• í›„ ì‹œë„í•˜ì„¸ìš”.")
            return

        retriever = self.vector_stores[db_name].as_retriever(search_kwargs={"k": 5})

        prompt_template = ChatPromptTemplate.from_template(f"""
        {self.system_prompt}
        
        ë¬¸ì„œ ë‚´ìš©:
        {{context}}

        ì‚¬ìš©ì ì§ˆë¬¸:
        {{question}}

        ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.
        """)

        qa_chain = RetrievalQA.from_chain_type(
            llm=self.llm,
            retriever=retriever,
            chain_type="stuff",
            return_source_documents=True,
            chain_type_kwargs={"prompt": prompt_template},
        )

        response = qa_chain.invoke(user_prompt)
        return response["result"]

    def handle_user_query(self, user_prompt):
        """ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë¶„ë¥˜ í›„ ì ì ˆí•œ ë°©ì‹ìœ¼ë¡œ ë‹µë³€"""
        query_type = self.classify_query(user_prompt)

        if query_type == "general":
            return self.llm.invoke(f"{self.system_prompt}\n\nì‚¬ìš©ì ì§ˆë¬¸: {user_prompt}")
        elif query_type == "operation":
            return self.query_rag(user_prompt, "operation")
        elif query_type == "education":
            return self.query_rag(user_prompt, "education")
        elif query_type == "lunch":
            return self.query_rag(user_prompt, "lunch")
        else:
            return ":x: ì§ˆë¬¸ì„ ì˜¬ë°”ë¥´ê²Œ ë¶„ë¥˜í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì…ë ¥í•´ì£¼ì„¸ìš”."


# ì‹¤í–‰ ì½”ë“œ ì˜ˆì‹œ
if __name__ == "__main__":
    chatbot = Chatbot()
    chatbot.set_system_prompt("ë‹¹ì‹ ì€ ì „ë¬¸ì ì¸ AI ë¹„ì„œì…ë‹ˆë‹¤.")

    chatbot.process_pdf_dir_to_chroma("./operation", "operation")
    chatbot.process_pdf_dir_to_chroma("./education", "education")

    user_prompt = input("ì‚¬ìš©ì ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”: ").strip()
    response = chatbot.handle_user_query(user_prompt)

    print("\n:memo: ë‹µë³€:")
    print(response)